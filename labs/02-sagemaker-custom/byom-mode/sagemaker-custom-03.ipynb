{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow BYOM: Treinando local e efetuando o deploy no Amazon SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução \n",
    "\n",
    "Vamos primeiramente treinar um modelo localmente, sem enviar o treinamento para o SageMaker. Após a criação do modelo, vamos efetuar o deploy no endpoint de inferência do Amazon SageMaker.\n",
    "\n",
    "Iremos utilizar o ``tensorflow.estimator.DNNClassifier`` no dataset [IRIS](https://archive.ics.uci.edu/ml/datasets/iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import boto3, re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from iris_dnn_classifier import estimator_fn\n",
    "from six.moves.urllib.request import urlopen\n",
    "from iris_dnn_classifier import train_input_fn\n",
    "from iris_dnn_classifier import serving_input_fn\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do Modelo\n",
    "\n",
    "Vamos utilizar o [``tensorflow.estimator.DNNClassifier``](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) como Estimator. Também precisamos escrever alguns métodos para fornecer entradas durante a hospedagem e o treinamento. Esses métodos são todos encontrados abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat iris_dnn_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = estimator_fn(run_config = None, params = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dos dados\n",
    "\n",
    "Em seguida, precisamos extrair os dados do repositório Tensorflow e torná-los prontos para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "if not os.path.exists(IRIS_TRAINING):\n",
    "    raw = urlopen(IRIS_TRAINING_URL).read()\n",
    "    with open(IRIS_TRAINING, \"wb\") as f:\n",
    "      f.write(raw)\n",
    "\n",
    "if not os.path.exists(IRIS_TEST):\n",
    "    raw = urlopen(IRIS_TEST_URL).read()\n",
    "    with open(IRIS_TEST, \"wb\") as f:\n",
    "      f.write(raw)\n",
    "\n",
    "train_func = train_input_fn('.', params = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn = train_func, steps = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o modelo para hospedagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportando o modelo do Tensorflow\n",
    "\n",
    "Para configurar o hosting, temos que importar o modelo do treinamento para o hosting. Começaremos exportando o modelo do TensorFlow e salvando-o, mas uma estrutura precisa ser seguida. O modelo exportado deve ser convertido em um formato legível por `` sagemaker.tensorflow.model.TensorFlowModel``. O código a seguir descreve a exportação do modelo:\n",
    "\n",
    "Há uma pequena diferença entre um modelo SageMaker e um modelo TensorFlow. A conversão é fácil e bastante trivial. Simplesmente mova o modelo exportado do tensorflow para um diretório `` export/Servo/`` e comprima em tar o diretório inteiro. O SageMaker reconhecerá isso como um modelo do TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model = classifier.export_savedmodel(export_dir_base = 'export/Servo/', \n",
    "                               serving_input_receiver_fn = serving_input_fn)\n",
    "\n",
    "print (exported_model)\n",
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o modelo no Amazon SageMaker\n",
    "\n",
    "Iremos utilizar o mesmo bucket da etapa anterior para armazenar nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket= 'sagemaker-workshop-files-us-east-1-206730628141' # troque pelo seu bucket (o bucket foi criado pelo CloudFormation, verifique a aba de output no CloudFormation)\n",
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, 'rb')\n",
    "    key = '{}/{}'.format(channel, file)\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3('tf-model','model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + bucket + '/tf-model/model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '1.12',\n",
    "                                  entry_point = 'iris_dnn_classifier.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o endpoint no Amazon SageMaker\n",
    "\n",
    "Podemo usar o ``sagemaker.tensorflow.model.TensorFlowModel.deploy`` como método para efetuar o deploy do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                          instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efetuando chamadas para o endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [6.4,3.2,4.5,1.5]\n",
    "predictor.predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclua todos os diretórios temporários para que não afetemos a próxima execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('model.tar.gz')\n",
    "import shutil\n",
    "shutil.rmtree('export')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você não quiser continuar usando o endpoint, pode removê-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
